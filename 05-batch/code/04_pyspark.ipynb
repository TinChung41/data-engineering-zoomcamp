{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07de9dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca5bbb06",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName('test') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf8de204",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'wget' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "# !wget https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2021-01.parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a52087c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'wc' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "# !wc -l fhvhv_tripdata_2021-01.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "931021a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .parquet('fhvhv_tripdata_2021-01.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "75456b35-9572-4a0a-a0f9-405fe24cca23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------------------+--------------------+-------------------+-------------------+-------------------+-------------------+------------+------------+----------+---------+-------------------+-----+----+---------+--------------------+-----------+----+----------+-------------------+-----------------+------------------+----------------+--------------+\n",
      "|hvfhs_license_num|dispatching_base_num|originating_base_num|   request_datetime|  on_scene_datetime|    pickup_datetime|   dropoff_datetime|PULocationID|DOLocationID|trip_miles|trip_time|base_passenger_fare|tolls| bcf|sales_tax|congestion_surcharge|airport_fee|tips|driver_pay|shared_request_flag|shared_match_flag|access_a_ride_flag|wav_request_flag|wav_match_flag|\n",
      "+-----------------+--------------------+--------------------+-------------------+-------------------+-------------------+-------------------+------------+------------+----------+---------+-------------------+-----+----+---------+--------------------+-----------+----+----------+-------------------+-----------------+------------------+----------------+--------------+\n",
      "|           HV0003|              B02682|              B02682|2021-01-01 00:28:09|2021-01-01 00:31:42|2021-01-01 00:33:44|2021-01-01 00:49:07|         230|         166|      5.26|      923|              22.28|  0.0|0.67|     1.98|                2.75|       NULL| 0.0|     14.99|                  N|                N|                  |               N|             N|\n",
      "|           HV0003|              B02682|              B02682|2021-01-01 00:45:56|2021-01-01 00:55:19|2021-01-01 00:55:19|2021-01-01 01:18:21|         152|         167|      3.65|     1382|              18.36|  0.0|0.55|     1.63|                 0.0|       NULL| 0.0|     17.06|                  N|                N|                  |               N|             N|\n",
      "|           HV0003|              B02764|              B02764|2021-01-01 00:21:15|2021-01-01 00:22:41|2021-01-01 00:23:56|2021-01-01 00:38:05|         233|         142|      3.51|      849|              14.05|  0.0|0.48|     1.25|                2.75|       NULL|0.94|     12.98|                  N|                N|                  |               N|             N|\n",
      "|           HV0003|              B02764|              B02764|2021-01-01 00:39:12|2021-01-01 00:42:37|2021-01-01 00:42:51|2021-01-01 00:45:50|         142|         143|      0.74|      179|               7.91|  0.0|0.24|      0.7|                2.75|       NULL| 0.0|      7.41|                  N|                N|                  |               N|             N|\n",
      "|           HV0003|              B02764|              B02764|2021-01-01 00:46:11|2021-01-01 00:47:17|2021-01-01 00:48:14|2021-01-01 01:08:42|         143|          78|       9.2|     1228|              27.11|  0.0|0.81|     2.41|                2.75|       NULL| 0.0|     22.44|                  N|                N|                  |               N|             N|\n",
      "|           HV0005|              B02510|                NULL|2021-01-01 00:04:00|               NULL|2021-01-01 00:06:59|2021-01-01 00:43:01|          88|          42|     9.725|     2162|              28.11|  0.0|0.84|     2.49|                2.75|       NULL| 0.0|      28.9|                  N|                N|                 N|               N|             N|\n",
      "|           HV0005|              B02510|                NULL|2021-01-01 00:40:06|               NULL|2021-01-01 00:50:00|2021-01-01 01:04:57|          42|         151|     2.469|      897|              25.03|  0.0|0.75|     2.22|                 0.0|       NULL| 0.0|     15.01|                  N|                N|                 N|               N|             N|\n",
      "|           HV0003|              B02764|              B02764|2021-01-01 00:10:36|2021-01-01 00:12:28|2021-01-01 00:14:30|2021-01-01 00:50:27|          71|         226|     13.53|     2157|              29.67|  0.0|1.04|     3.08|                 0.0|       NULL| 0.0|      34.2|                  N|                N|                  |               N|             N|\n",
      "|           HV0003|              B02875|              B02875|2021-01-01 00:21:17|2021-01-01 00:22:25|2021-01-01 00:22:54|2021-01-01 00:30:20|         112|         255|       1.6|      446|               6.89|  0.0|0.21|     0.61|                 0.0|       NULL| 0.0|      6.26|                  N|                N|                  |               N|             N|\n",
      "|           HV0003|              B02875|              B02875|2021-01-01 00:36:57|2021-01-01 00:38:09|2021-01-01 00:40:12|2021-01-01 00:53:31|         255|         232|       3.2|      800|              11.51|  0.0|0.53|     1.03|                2.75|       NULL|2.82|     10.99|                  N|                N|                  |               N|             N|\n",
      "|           HV0003|              B02875|              B02875|2021-01-01 00:53:31|2021-01-01 00:56:21|2021-01-01 00:56:45|2021-01-01 01:17:42|         232|         198|      5.74|     1257|              17.18|  0.0|0.52|     1.52|                2.75|       NULL| 0.0|     17.61|                  N|                N|                  |               N|             N|\n",
      "|           HV0003|              B02835|              B02835|2021-01-01 00:22:58|2021-01-01 00:27:01|2021-01-01 00:29:04|2021-01-01 00:36:27|         113|          48|       1.8|      443|               8.18|  0.0|0.25|     0.73|                2.75|       NULL| 0.0|      6.12|                  N|                N|                  |               N|             N|\n",
      "|           HV0003|              B02835|              B02835|2021-01-01 00:46:44|2021-01-01 00:47:49|2021-01-01 00:48:56|2021-01-01 00:59:12|         239|          75|       2.9|      616|               13.1|  0.0|0.45|     1.17|                2.75|       NULL|0.94|      8.77|                  N|                N|                  |               N|             N|\n",
      "|           HV0004|              B02800|                NULL|2021-01-01 00:12:50|               NULL|2021-01-01 00:15:24|2021-01-01 00:38:31|         181|         237|      9.66|     1387|              32.95|  0.0| 0.0|     2.34|                2.75|       NULL| 0.0|      21.1|                  N|                N|                 N|               N|             N|\n",
      "|           HV0004|              B02800|                NULL|2021-01-01 00:35:32|               NULL|2021-01-01 00:45:00|2021-01-01 01:06:45|         236|          68|      4.38|     1305|              22.91|  0.0| 0.0|     1.63|                2.75|       NULL|3.43|     15.82|                  N|                N|                 N|               N|             N|\n",
      "|           HV0003|              B02682|              B02682|2021-01-01 00:10:22|2021-01-01 00:11:03|2021-01-01 00:11:53|2021-01-01 00:18:06|         256|         148|      2.03|      373|               7.84|  0.0|0.42|      0.7|                2.75|       NULL|2.82|      6.93|                  N|                N|                  |               N|             N|\n",
      "|           HV0003|              B02682|              B02682|2021-01-01 00:25:00|2021-01-01 00:26:31|2021-01-01 00:28:31|2021-01-01 00:41:40|          79|          80|      3.08|      789|               13.2|  0.0| 0.4|     1.17|                2.75|       NULL| 0.0|     11.54|                  N|                N|                  |               N|             N|\n",
      "|           HV0003|              B02682|              B02682|2021-01-01 00:44:56|2021-01-01 00:49:55|2021-01-01 00:50:49|2021-01-01 00:55:59|          17|         217|      1.17|      310|               7.91|  0.0|0.24|      0.7|                 0.0|       NULL| 0.0|      6.94|                  N|                N|                  |               N|             N|\n",
      "|           HV0005|              B02510|                NULL|2021-01-01 00:05:04|               NULL|2021-01-01 00:08:40|2021-01-01 00:39:39|          62|          29|    10.852|     1859|              31.18|  0.0|0.94|     2.77|                 0.0|       NULL| 0.0|     27.61|                  N|                N|                 N|               N|             N|\n",
      "|           HV0003|              B02836|              B02836|2021-01-01 00:40:44|2021-01-01 00:53:34|2021-01-01 00:53:48|2021-01-01 01:11:40|          22|          22|      3.52|     1072|              28.67|  0.0|0.86|     2.54|                 0.0|       NULL| 0.0|     17.64|                  N|                N|                  |               N|             N|\n",
      "+-----------------+--------------------+--------------------+-------------------+-------------------+-------------------+-------------------+------------+------------+----------+---------+-------------------+-----+----+---------+--------------------+-----------+----+----------+-------------------+-----------------+------------------+----------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d44b7839",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('hvfhs_license_num', StringType(), True), StructField('dispatching_base_num', StringType(), True), StructField('originating_base_num', StringType(), True), StructField('request_datetime', TimestampNTZType(), True), StructField('on_scene_datetime', TimestampNTZType(), True), StructField('pickup_datetime', TimestampNTZType(), True), StructField('dropoff_datetime', TimestampNTZType(), True), StructField('PULocationID', LongType(), True), StructField('DOLocationID', LongType(), True), StructField('trip_miles', DoubleType(), True), StructField('trip_time', LongType(), True), StructField('base_passenger_fare', DoubleType(), True), StructField('tolls', DoubleType(), True), StructField('bcf', DoubleType(), True), StructField('sales_tax', DoubleType(), True), StructField('congestion_surcharge', DoubleType(), True), StructField('airport_fee', DoubleType(), True), StructField('tips', DoubleType(), True), StructField('driver_pay', DoubleType(), True), StructField('shared_request_flag', StringType(), True), StructField('shared_match_flag', StringType(), True), StructField('access_a_ride_flag', StringType(), True), StructField('wav_request_flag', StringType(), True), StructField('wav_match_flag', StringType(), True)])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4249e790",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'head' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "!head -n 1001 fhvhv_tripdata_2021-01.csv > head.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6894312c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f3ca771b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pandas = pd.read_parquet('fhvhv_tripdata_2021-01.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d6a00f4b-aa70-4c93-b927-d571bae580ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pandas = df_pandas.head(1001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f1066b4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "hvfhs_license_num               object\n",
       "dispatching_base_num            object\n",
       "originating_base_num            object\n",
       "request_datetime        datetime64[ns]\n",
       "on_scene_datetime       datetime64[ns]\n",
       "pickup_datetime         datetime64[ns]\n",
       "dropoff_datetime        datetime64[ns]\n",
       "PULocationID                     int64\n",
       "DOLocationID                     int64\n",
       "trip_miles                     float64\n",
       "trip_time                        int64\n",
       "base_passenger_fare            float64\n",
       "tolls                          float64\n",
       "bcf                            float64\n",
       "sales_tax                      float64\n",
       "congestion_surcharge           float64\n",
       "airport_fee                    float64\n",
       "tips                           float64\n",
       "driver_pay                     float64\n",
       "shared_request_flag             object\n",
       "shared_match_flag               object\n",
       "access_a_ride_flag              object\n",
       "wav_request_flag                object\n",
       "wav_match_flag                  object\n",
       "dtype: object"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pandas.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a42be32f-10ec-4f91-a3c7-9d8431c1d3af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "hvfhs_license_num               object\n",
       "dispatching_base_num            object\n",
       "pickup_datetime         datetime64[ns]\n",
       "dropoff_datetime        datetime64[ns]\n",
       "PULocationID                     int64\n",
       "DOLocationID                     int64\n",
       "SR_Flag                         object\n",
       "dtype: object"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pandas = df_pandas[['hvfhs_license_num', 'dispatching_base_num', 'pickup_datetime', 'dropoff_datetime', 'PULocationID', 'DOLocationID', 'shared_match_flag']]\n",
    "\n",
    "# Rename the 'shared_match_flag' column to 'SR_Flag'\n",
    "df_pandas.rename(columns={'shared_match_flag': 'SR_Flag'}, inplace=True)\n",
    "\n",
    "df_pandas.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "adaf10f9-199f-4c58-b2dd-1b5fdf052997",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1001, 7)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pandas.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f8413c9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('hvfhs_license_num', StringType(), True), StructField('dispatching_base_num', StringType(), True), StructField('pickup_datetime', TimestampType(), True), StructField('dropoff_datetime', TimestampType(), True), StructField('PULocationID', LongType(), True), StructField('DOLocationID', LongType(), True), StructField('SR_Flag', StringType(), True)])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.createDataFrame(df_pandas).schema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f252c1",
   "metadata": {},
   "source": [
    "Integer - 4 bytes\n",
    "Long - 8 bytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "16937bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fc61a99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = types.StructType([\n",
    "    types.StructField('hvfhs_license_num', types.StringType(), True),\n",
    "    types.StructField('dispatching_base_num', types.StringType(), True),\n",
    "    types.StructField('pickup_datetime', types.TimestampType(), True),\n",
    "    types.StructField('dropoff_datetime', types.TimestampType(), True),\n",
    "    types.StructField('PULocationID', types.LongType(), True),\n",
    "    types.StructField('DOLocationID', types.LongType(), True),\n",
    "    types.StructField('SR_Flag', types.StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f94052ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = spark.read \\\n",
    "#     .option(\"header\", \"true\") \\\n",
    "#     .schema(schema) \\\n",
    "#     .csv('fhvhv_tripdata_2021-01.csv')\n",
    "\n",
    "df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .schema(schema) \\\n",
    "    .parquet('fhvhv_tripdata_2021-01.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c270d9d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.repartition(24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7796c2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.parquet('fhvhv/2021/01/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c3cab876",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.parquet('fhvhv/2021/01/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a281333a-f008-40d9-a0a0-b1650384ba0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[hvfhs_license_num: string, dispatching_base_num: string, pickup_datetime: timestamp, dropoff_datetime: timestamp, PULocationID: bigint, DOLocationID: bigint, SR_Flag: string]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "203b5627",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- hvfhs_license_num: string (nullable = true)\n",
      " |-- dispatching_base_num: string (nullable = true)\n",
      " |-- pickup_datetime: timestamp (nullable = true)\n",
      " |-- dropoff_datetime: timestamp (nullable = true)\n",
      " |-- PULocationID: long (nullable = true)\n",
      " |-- DOLocationID: long (nullable = true)\n",
      " |-- SR_Flag: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64172a47",
   "metadata": {},
   "source": [
    "SELECT * FROM df WHERE hvfhs_license_num =  HV0003"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d24840a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2470cf6-ef6f-4487-bbbd-e3454ae7b564",
   "metadata": {},
   "outputs": [],
   "source": [
    "F."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3ab1ca44",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------+------------+------------+\n",
      "|pickup_date|dropoff_date|PULocationID|DOLocationID|\n",
      "+-----------+------------+------------+------------+\n",
      "| 2021-01-30|  2021-01-30|         256|         256|\n",
      "| 2021-01-13|  2021-01-13|          61|          76|\n",
      "| 2021-01-30|  2021-01-30|         113|         262|\n",
      "| 2021-01-23|  2021-01-23|          22|         227|\n",
      "| 2021-01-29|  2021-01-29|         198|          95|\n",
      "| 2021-01-16|  2021-01-16|         235|          41|\n",
      "| 2021-01-11|  2021-01-11|         138|         223|\n",
      "| 2021-01-29|  2021-01-29|         155|          89|\n",
      "| 2021-01-04|  2021-01-04|          26|         227|\n",
      "| 2021-01-12|  2021-01-12|         221|         172|\n",
      "| 2021-01-25|  2021-01-25|         258|         181|\n",
      "| 2021-01-14|  2021-01-14|         140|          72|\n",
      "| 2021-01-21|  2021-01-21|          49|         217|\n",
      "| 2021-01-05|  2021-01-06|         169|         159|\n",
      "| 2021-01-21|  2021-01-21|         235|          69|\n",
      "| 2021-01-23|  2021-01-23|         146|          37|\n",
      "| 2021-01-07|  2021-01-07|         165|         165|\n",
      "| 2021-01-03|  2021-01-03|         243|          24|\n",
      "| 2021-01-15|  2021-01-15|         239|         262|\n",
      "| 2021-01-26|  2021-01-26|         170|         249|\n",
      "+-----------+------------+------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df\\\n",
    "    .withColumn('pickup_date', F.to_date(df.pickup_datetime))\\\n",
    "    .withColumn('dropoff_date', F.to_date(df.dropoff_datetime))\\\n",
    "    .select('pickup_date','dropoff_date','PULocationID', 'DOLocationID') \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6d98c2ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crazy_stuff(base_num):\n",
    "    num = int(base_num[1:])\n",
    "    if num % 7 == 0:\n",
    "        return f's/{num:03x}'\n",
    "    elif num % 3 == 0:\n",
    "        return f'a/{num:03x}'\n",
    "    else:\n",
    "        return f'e/{num:03x}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f3175419",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s/b44'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crazy_stuff('B02884')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9bb5d503",
   "metadata": {},
   "outputs": [],
   "source": [
    "crazy_stuff_udf = F.udf(crazy_stuff, returnType=types.StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "bf831855-8232-4f2e-b1a6-b50662ed1ce4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------------------+-------------------+-------------------+------------+------------+-------+\n",
      "|hvfhs_license_num|dispatching_base_num|    pickup_datetime|   dropoff_datetime|PULocationID|DOLocationID|SR_Flag|\n",
      "+-----------------+--------------------+-------------------+-------------------+------------+------------+-------+\n",
      "|           HV0003|              B02872|2021-01-30 01:41:29|2021-01-30 01:51:44|         256|         256|   NULL|\n",
      "|           HV0005|              B02510|2021-01-13 01:39:57|2021-01-13 02:00:30|          61|          76|   NULL|\n",
      "|           HV0003|              B02765|2021-01-30 02:25:19|2021-01-30 02:41:49|         113|         262|   NULL|\n",
      "|           HV0005|              B02510|2021-01-23 03:08:47|2021-01-23 03:20:16|          22|         227|   NULL|\n",
      "|           HV0003|              B02765|2021-01-29 21:13:39|2021-01-29 21:28:50|         198|          95|   NULL|\n",
      "|           HV0003|              B02887|2021-01-16 17:11:49|2021-01-16 17:24:02|         235|          41|   NULL|\n",
      "|           HV0003|              B02869|2021-01-11 20:04:13|2021-01-11 20:11:39|         138|         223|   NULL|\n",
      "|           HV0003|              B02872|2021-01-29 04:54:17|2021-01-29 05:08:09|         155|          89|   NULL|\n",
      "|           HV0003|              B02617|2021-01-04 23:43:38|2021-01-04 23:47:48|          26|         227|   NULL|\n",
      "|           HV0003|              B02617|2021-01-12 11:57:34|2021-01-12 12:15:53|         221|         172|   NULL|\n",
      "|           HV0003|              B02872|2021-01-25 17:11:13|2021-01-25 17:39:33|         258|         181|   NULL|\n",
      "|           HV0003|              B02883|2021-01-14 04:32:29|2021-01-14 05:15:28|         140|          72|   NULL|\n",
      "|           HV0003|              B02765|2021-01-21 01:16:52|2021-01-21 01:31:39|          49|         217|   NULL|\n",
      "|           HV0003|              B02883|2021-01-05 23:54:13|2021-01-06 00:16:26|         169|         159|   NULL|\n",
      "|           HV0005|              B02510|2021-01-21 02:39:55|2021-01-21 02:59:26|         235|          69|   NULL|\n",
      "|           HV0003|              B02864|2021-01-23 00:37:04|2021-01-23 01:05:14|         146|          37|   NULL|\n",
      "|           HV0005|              B02510|2021-01-07 23:05:20|2021-01-07 23:10:16|         165|         165|   NULL|\n",
      "|           HV0003|              B02882|2021-01-03 22:11:11|2021-01-03 22:30:16|         243|          24|   NULL|\n",
      "|           HV0003|              B02872|2021-01-15 00:18:55|2021-01-15 00:29:45|         239|         262|   NULL|\n",
      "|           HV0005|              B02510|2021-01-26 19:07:56|2021-01-26 19:22:40|         170|         249|   NULL|\n",
      "+-----------------+--------------------+-------------------+-------------------+------------+------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b38f0465",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o177.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 16.0 failed 1 times, most recent failure: Lost task 0.0 in stage 16.0 (TID 69) (WIN-5VTIC6KQEVL executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\r\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec.evaluate(BatchEvalPythonExec.scala:54)\r\n\tat org.apache.spark.sql.execution.python.EvalPythonExec.$anonfun$doExecute$2(EvalPythonExec.scala:131)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:855)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:855)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\r\n\tat java.base/java.lang.Thread.run(Thread.java:833)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/sun.nio.ch.NioSocketImpl.timedAccept(NioSocketImpl.java:708)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.accept(NioSocketImpl.java:752)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:675)\r\n\tat java.base/java.net.ServerSocket.platformImplAccept(ServerSocket.java:641)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:617)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:574)\r\n\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:532)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)\r\n\t... 27 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:984)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:530)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)\r\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)\r\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4344)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3326)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4334)\r\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4332)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4332)\r\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3326)\r\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3549)\r\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:280)\r\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:315)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:833)\r\nCaused by: org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\r\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec.evaluate(BatchEvalPythonExec.scala:54)\r\n\tat org.apache.spark.sql.execution.python.EvalPythonExec.$anonfun$doExecute$2(EvalPythonExec.scala:131)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:855)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:855)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\r\n\t... 1 more\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/sun.nio.ch.NioSocketImpl.timedAccept(NioSocketImpl.java:708)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.accept(NioSocketImpl.java:752)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:675)\r\n\tat java.base/java.net.ServerSocket.platformImplAccept(ServerSocket.java:641)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:617)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:574)\r\n\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:532)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)\r\n\t... 27 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Input \u001b[1;32mIn [43]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mdf\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwithColumn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpickup_date\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_date\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpickup_datetime\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwithColumn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdropoff_date\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_date\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropoff_datetime\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwithColumn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbase_id\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcrazy_stuff_udf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdispatching_base_num\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbase_id\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpickup_date\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdropoff_date\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mPULocationID\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mDOLocationID\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pyspark\\sql\\dataframe.py:959\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[1;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[0;32m    953\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[0;32m    954\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_BOOL\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    955\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvertical\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(vertical)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[0;32m    956\u001b[0m     )\n\u001b[0;32m    958\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(truncate, \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m truncate:\n\u001b[1;32m--> 959\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshowString\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    960\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    961\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o177.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 16.0 failed 1 times, most recent failure: Lost task 0.0 in stage 16.0 (TID 69) (WIN-5VTIC6KQEVL executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\r\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec.evaluate(BatchEvalPythonExec.scala:54)\r\n\tat org.apache.spark.sql.execution.python.EvalPythonExec.$anonfun$doExecute$2(EvalPythonExec.scala:131)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:855)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:855)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\r\n\tat java.base/java.lang.Thread.run(Thread.java:833)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/sun.nio.ch.NioSocketImpl.timedAccept(NioSocketImpl.java:708)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.accept(NioSocketImpl.java:752)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:675)\r\n\tat java.base/java.net.ServerSocket.platformImplAccept(ServerSocket.java:641)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:617)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:574)\r\n\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:532)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)\r\n\t... 27 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:984)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:530)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)\r\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)\r\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4344)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3326)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4334)\r\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4332)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4332)\r\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3326)\r\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3549)\r\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:280)\r\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:315)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:833)\r\nCaused by: org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\r\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec.evaluate(BatchEvalPythonExec.scala:54)\r\n\tat org.apache.spark.sql.execution.python.EvalPythonExec.$anonfun$doExecute$2(EvalPythonExec.scala:131)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:855)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:855)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\r\n\t... 1 more\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/sun.nio.ch.NioSocketImpl.timedAccept(NioSocketImpl.java:708)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.accept(NioSocketImpl.java:752)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:675)\r\n\tat java.base/java.net.ServerSocket.platformImplAccept(ServerSocket.java:641)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:617)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:574)\r\n\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:532)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)\r\n\t... 27 more\r\n"
     ]
    }
   ],
   "source": [
    "df \\\n",
    "    .withColumn('pickup_date', F.to_date(df.pickup_datetime)) \\\n",
    "    .withColumn('dropoff_date', F.to_date(df.dropoff_datetime)) \\\n",
    "    .withColumn('base_id', crazy_stuff_udf(df.dispatching_base_num)) \\\n",
    "    .select('base_id', 'pickup_date', 'dropoff_date', 'PULocationID', 'DOLocationID') \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "00921644",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(pickup_datetime=datetime.datetime(2021, 1, 30, 1, 41, 29), dropoff_datetime=datetime.datetime(2021, 1, 30, 1, 51, 44), PULocationID=256, DOLocationID=256),\n",
       " Row(pickup_datetime=datetime.datetime(2021, 1, 30, 2, 25, 19), dropoff_datetime=datetime.datetime(2021, 1, 30, 2, 41, 49), PULocationID=113, DOLocationID=262),\n",
       " Row(pickup_datetime=datetime.datetime(2021, 1, 29, 21, 13, 39), dropoff_datetime=datetime.datetime(2021, 1, 29, 21, 28, 50), PULocationID=198, DOLocationID=95),\n",
       " Row(pickup_datetime=datetime.datetime(2021, 1, 16, 17, 11, 49), dropoff_datetime=datetime.datetime(2021, 1, 16, 17, 24, 2), PULocationID=235, DOLocationID=41),\n",
       " Row(pickup_datetime=datetime.datetime(2021, 1, 11, 20, 4, 13), dropoff_datetime=datetime.datetime(2021, 1, 11, 20, 11, 39), PULocationID=138, DOLocationID=223),\n",
       " Row(pickup_datetime=datetime.datetime(2021, 1, 29, 4, 54, 17), dropoff_datetime=datetime.datetime(2021, 1, 29, 5, 8, 9), PULocationID=155, DOLocationID=89),\n",
       " Row(pickup_datetime=datetime.datetime(2021, 1, 4, 23, 43, 38), dropoff_datetime=datetime.datetime(2021, 1, 4, 23, 47, 48), PULocationID=26, DOLocationID=227),\n",
       " Row(pickup_datetime=datetime.datetime(2021, 1, 12, 11, 57, 34), dropoff_datetime=datetime.datetime(2021, 1, 12, 12, 15, 53), PULocationID=221, DOLocationID=172),\n",
       " Row(pickup_datetime=datetime.datetime(2021, 1, 25, 17, 11, 13), dropoff_datetime=datetime.datetime(2021, 1, 25, 17, 39, 33), PULocationID=258, DOLocationID=181),\n",
       " Row(pickup_datetime=datetime.datetime(2021, 1, 14, 4, 32, 29), dropoff_datetime=datetime.datetime(2021, 1, 14, 5, 15, 28), PULocationID=140, DOLocationID=72),\n",
       " Row(pickup_datetime=datetime.datetime(2021, 1, 21, 1, 16, 52), dropoff_datetime=datetime.datetime(2021, 1, 21, 1, 31, 39), PULocationID=49, DOLocationID=217),\n",
       " Row(pickup_datetime=datetime.datetime(2021, 1, 5, 23, 54, 13), dropoff_datetime=datetime.datetime(2021, 1, 6, 0, 16, 26), PULocationID=169, DOLocationID=159),\n",
       " Row(pickup_datetime=datetime.datetime(2021, 1, 23, 0, 37, 4), dropoff_datetime=datetime.datetime(2021, 1, 23, 1, 5, 14), PULocationID=146, DOLocationID=37),\n",
       " Row(pickup_datetime=datetime.datetime(2021, 1, 3, 22, 11, 11), dropoff_datetime=datetime.datetime(2021, 1, 3, 22, 30, 16), PULocationID=243, DOLocationID=24),\n",
       " Row(pickup_datetime=datetime.datetime(2021, 1, 15, 0, 18, 55), dropoff_datetime=datetime.datetime(2021, 1, 15, 0, 29, 45), PULocationID=239, DOLocationID=262),\n",
       " Row(pickup_datetime=datetime.datetime(2021, 1, 13, 4, 23, 28), dropoff_datetime=datetime.datetime(2021, 1, 13, 4, 34, 1), PULocationID=37, DOLocationID=36),\n",
       " Row(pickup_datetime=datetime.datetime(2021, 1, 30, 0, 56, 31), dropoff_datetime=datetime.datetime(2021, 1, 30, 1, 3, 10), PULocationID=39, DOLocationID=39),\n",
       " Row(pickup_datetime=datetime.datetime(2021, 1, 15, 15, 29, 15), dropoff_datetime=datetime.datetime(2021, 1, 15, 15, 40, 39), PULocationID=133, DOLocationID=89),\n",
       " Row(pickup_datetime=datetime.datetime(2021, 1, 17, 5, 36, 46), dropoff_datetime=datetime.datetime(2021, 1, 17, 5, 48, 45), PULocationID=91, DOLocationID=35),\n",
       " Row(pickup_datetime=datetime.datetime(2021, 1, 23, 0, 1, 26), dropoff_datetime=datetime.datetime(2021, 1, 23, 0, 30, 26), PULocationID=69, DOLocationID=243),\n",
       " Row(pickup_datetime=datetime.datetime(2021, 1, 6, 21, 50, 32), dropoff_datetime=datetime.datetime(2021, 1, 6, 22, 10, 52), PULocationID=65, DOLocationID=145),\n",
       " Row(pickup_datetime=datetime.datetime(2021, 1, 16, 23, 14, 12), dropoff_datetime=datetime.datetime(2021, 1, 16, 23, 28, 27), PULocationID=113, DOLocationID=145),\n",
       " Row(pickup_datetime=datetime.datetime(2021, 1, 21, 18, 55, 35), dropoff_datetime=datetime.datetime(2021, 1, 21, 19, 1, 45), PULocationID=225, DOLocationID=17),\n",
       " Row(pickup_datetime=datetime.datetime(2021, 1, 19, 6, 43, 1), dropoff_datetime=datetime.datetime(2021, 1, 19, 7, 1, 15), PULocationID=213, DOLocationID=94),\n",
       " Row(pickup_datetime=datetime.datetime(2021, 1, 3, 19, 40, 52), dropoff_datetime=datetime.datetime(2021, 1, 3, 19, 54, 1), PULocationID=70, DOLocationID=223),\n",
       " Row(pickup_datetime=datetime.datetime(2021, 1, 6, 2, 15, 25), dropoff_datetime=datetime.datetime(2021, 1, 6, 2, 37, 9), PULocationID=138, DOLocationID=140),\n",
       " Row(pickup_datetime=datetime.datetime(2021, 1, 14, 0, 26, 58), dropoff_datetime=datetime.datetime(2021, 1, 14, 0, 31, 46), PULocationID=75, DOLocationID=74),\n",
       " Row(pickup_datetime=datetime.datetime(2021, 1, 24, 20, 0, 18), dropoff_datetime=datetime.datetime(2021, 1, 24, 20, 8, 52), PULocationID=159, DOLocationID=147),\n",
       " Row(pickup_datetime=datetime.datetime(2021, 1, 10, 7, 43, 30), dropoff_datetime=datetime.datetime(2021, 1, 10, 8, 12, 51), PULocationID=213, DOLocationID=213),\n",
       " Row(pickup_datetime=datetime.datetime(2021, 1, 30, 5, 38, 25), dropoff_datetime=datetime.datetime(2021, 1, 30, 5, 46, 8), PULocationID=243, DOLocationID=265),\n",
       " Row(pickup_datetime=datetime.datetime(2021, 1, 16, 13, 0, 47), dropoff_datetime=datetime.datetime(2021, 1, 16, 13, 22, 20), PULocationID=220, DOLocationID=142),\n",
       " Row(pickup_datetime=datetime.datetime(2021, 1, 18, 3, 52, 33), dropoff_datetime=datetime.datetime(2021, 1, 18, 4, 14, 23), PULocationID=231, DOLocationID=37),\n",
       " Row(pickup_datetime=datetime.datetime(2021, 1, 1, 10, 2, 7), dropoff_datetime=datetime.datetime(2021, 1, 1, 10, 7, 40), PULocationID=166, DOLocationID=74),\n",
       " Row(pickup_datetime=datetime.datetime(2021, 1, 19, 14, 35, 15), dropoff_datetime=datetime.datetime(2021, 1, 19, 14, 47, 35), PULocationID=68, DOLocationID=163),\n",
       " Row(pickup_datetime=datetime.datetime(2021, 1, 16, 8, 12, 32), dropoff_datetime=datetime.datetime(2021, 1, 16, 8, 27, 25), PULocationID=231, DOLocationID=224),\n",
       " Row(pickup_datetime=datetime.datetime(2021, 1, 10, 0, 3, 32), dropoff_datetime=datetime.datetime(2021, 1, 10, 0, 14, 7), PULocationID=85, DOLocationID=89),\n",
       " Row(pickup_datetime=datetime.datetime(2021, 1, 1, 11, 39, 8), dropoff_datetime=datetime.datetime(2021, 1, 1, 11, 46, 9), PULocationID=159, DOLocationID=116),\n",
       " Row(pickup_datetime=datetime.datetime(2021, 1, 3, 20, 38, 27), dropoff_datetime=datetime.datetime(2021, 1, 3, 20, 50, 5), PULocationID=134, DOLocationID=196),\n",
       " Row(pickup_datetime=datetime.datetime(2021, 1, 13, 17, 0, 38), dropoff_datetime=datetime.datetime(2021, 1, 13, 17, 30, 38), PULocationID=174, DOLocationID=234),\n",
       " Row(pickup_datetime=datetime.datetime(2021, 1, 26, 2, 58, 25), dropoff_datetime=datetime.datetime(2021, 1, 26, 3, 9, 37), PULocationID=198, DOLocationID=37),\n",
       " Row(pickup_datetime=datetime.datetime(2021, 1, 6, 17, 8, 51), dropoff_datetime=datetime.datetime(2021, 1, 6, 17, 27, 23), PULocationID=145, DOLocationID=90),\n",
       " Row(pickup_datetime=datetime.datetime(2021, 1, 15, 15, 34, 7), dropoff_datetime=datetime.datetime(2021, 1, 15, 15, 43, 40), PULocationID=181, DOLocationID=257),\n",
       " Row(pickup_datetime=datetime.datetime(2021, 1, 19, 7, 42, 57), dropoff_datetime=datetime.datetime(2021, 1, 19, 8, 2, 59), PULocationID=41, DOLocationID=41),\n",
       " Row(pickup_datetime=datetime.datetime(2021, 1, 12, 13, 54, 3), dropoff_datetime=datetime.datetime(2021, 1, 12, 14, 2, 16), PULocationID=74, DOLocationID=140),\n",
       " Row(pickup_datetime=datetime.datetime(2021, 1, 2, 5, 10, 12), dropoff_datetime=datetime.datetime(2021, 1, 2, 5, 26, 42), PULocationID=129, DOLocationID=129),\n",
       " Row(pickup_datetime=datetime.datetime(2021, 1, 15, 5, 43, 36), dropoff_datetime=datetime.datetime(2021, 1, 15, 5, 50, 48), PULocationID=80, DOLocationID=225),\n",
       " Row(pickup_datetime=datetime.datetime(2021, 1, 31, 4, 19, 24), dropoff_datetime=datetime.datetime(2021, 1, 31, 4, 25, 3), PULocationID=76, DOLocationID=63),\n",
       " Row(pickup_datetime=datetime.datetime(2021, 1, 3, 1, 54, 42), dropoff_datetime=datetime.datetime(2021, 1, 3, 2, 24, 14), PULocationID=78, DOLocationID=265),\n",
       " Row(pickup_datetime=datetime.datetime(2021, 1, 6, 0, 56, 39), dropoff_datetime=datetime.datetime(2021, 1, 6, 1, 2, 46), PULocationID=61, DOLocationID=17),\n",
       " Row(pickup_datetime=datetime.datetime(2021, 1, 9, 2, 14, 9), dropoff_datetime=datetime.datetime(2021, 1, 9, 3, 11, 34), PULocationID=225, DOLocationID=265),\n",
       " Row(pickup_datetime=datetime.datetime(2021, 1, 18, 7, 34, 8), dropoff_datetime=datetime.datetime(2021, 1, 18, 7, 38, 30), PULocationID=112, DOLocationID=112),\n",
       " Row(pickup_datetime=datetime.datetime(2021, 1, 8, 5, 42, 41), dropoff_datetime=datetime.datetime(2021, 1, 8, 5, 52, 28), PULocationID=212, DOLocationID=213),\n",
       " Row(pickup_datetime=datetime.datetime(2021, 1, 26, 18, 5, 11), dropoff_datetime=datetime.datetime(2021, 1, 26, 18, 20, 54), PULocationID=219, DOLocationID=197),\n",
       " Row(pickup_datetime=datetime.datetime(2021, 1, 13, 6, 9, 21), dropoff_datetime=datetime.datetime(2021, 1, 13, 6, 18, 12), PULocationID=17, DOLocationID=225),\n",
       " Row(pickup_datetime=datetime.datetime(2021, 1, 7, 15, 54, 42), dropoff_datetime=datetime.datetime(2021, 1, 7, 16, 3, 23), PULocationID=143, DOLocationID=151),\n",
       " Row(pickup_datetime=datetime.datetime(2021, 1, 9, 1, 56, 32), dropoff_datetime=datetime.datetime(2021, 1, 9, 2, 2, 35), PULocationID=71, DOLocationID=85),\n",
       " Row(pickup_datetime=datetime.datetime(2021, 1, 22, 19, 10, 53), dropoff_datetime=datetime.datetime(2021, 1, 22, 19, 36, 47), PULocationID=158, DOLocationID=140),\n",
       " Row(pickup_datetime=datetime.datetime(2021, 1, 9, 4, 58, 31), dropoff_datetime=datetime.datetime(2021, 1, 9, 5, 4, 30), PULocationID=144, DOLocationID=231),\n",
       " Row(pickup_datetime=datetime.datetime(2021, 1, 17, 14, 6, 24), dropoff_datetime=datetime.datetime(2021, 1, 17, 14, 37, 55), PULocationID=22, DOLocationID=265),\n",
       " Row(pickup_datetime=datetime.datetime(2021, 1, 24, 0, 42, 8), dropoff_datetime=datetime.datetime(2021, 1, 24, 0, 51, 33), PULocationID=36, DOLocationID=198),\n",
       " Row(pickup_datetime=datetime.datetime(2021, 1, 14, 23, 18, 57), dropoff_datetime=datetime.datetime(2021, 1, 14, 23, 30, 5), PULocationID=170, DOLocationID=229),\n",
       " Row(pickup_datetime=datetime.datetime(2021, 1, 25, 1, 46, 3), dropoff_datetime=datetime.datetime(2021, 1, 25, 1, 50, 54), PULocationID=208, DOLocationID=208),\n",
       " Row(pickup_datetime=datetime.datetime(2021, 1, 24, 3, 41, 20), dropoff_datetime=datetime.datetime(2021, 1, 24, 4, 1, 8), PULocationID=70, DOLocationID=37),\n",
       " Row(pickup_datetime=datetime.datetime(2021, 1, 31, 1, 43, 3), dropoff_datetime=datetime.datetime(2021, 1, 31, 2, 2, 34), PULocationID=17, DOLocationID=36),\n",
       " Row(pickup_datetime=datetime.datetime(2021, 1, 11, 20, 22, 14), dropoff_datetime=datetime.datetime(2021, 1, 11, 20, 26, 8), PULocationID=206, DOLocationID=206),\n",
       " Row(pickup_datetime=datetime.datetime(2021, 1, 6, 4, 32, 19), dropoff_datetime=datetime.datetime(2021, 1, 6, 4, 53, 23), PULocationID=239, DOLocationID=32),\n",
       " Row(pickup_datetime=datetime.datetime(2021, 1, 9, 2, 34, 59), dropoff_datetime=datetime.datetime(2021, 1, 9, 2, 46, 27), PULocationID=41, DOLocationID=168),\n",
       " Row(pickup_datetime=datetime.datetime(2021, 1, 18, 5, 37, 52), dropoff_datetime=datetime.datetime(2021, 1, 18, 5, 49, 46), PULocationID=260, DOLocationID=95),\n",
       " Row(pickup_datetime=datetime.datetime(2021, 1, 30, 0, 38, 38), dropoff_datetime=datetime.datetime(2021, 1, 30, 0, 59, 29), PULocationID=200, DOLocationID=185),\n",
       " Row(pickup_datetime=datetime.datetime(2021, 1, 10, 2, 3, 8), dropoff_datetime=datetime.datetime(2021, 1, 10, 2, 8, 5), PULocationID=29, DOLocationID=150),\n",
       " Row(pickup_datetime=datetime.datetime(2021, 1, 17, 4, 39, 2), dropoff_datetime=datetime.datetime(2021, 1, 17, 4, 54, 16), PULocationID=130, DOLocationID=265),\n",
       " Row(pickup_datetime=datetime.datetime(2021, 1, 4, 1, 29, 42), dropoff_datetime=datetime.datetime(2021, 1, 4, 1, 47, 59), PULocationID=75, DOLocationID=127),\n",
       " Row(pickup_datetime=datetime.datetime(2021, 1, 5, 13, 34, 59), dropoff_datetime=datetime.datetime(2021, 1, 5, 13, 44, 8), PULocationID=32, DOLocationID=212),\n",
       " Row(pickup_datetime=datetime.datetime(2021, 1, 14, 17, 18, 7), dropoff_datetime=datetime.datetime(2021, 1, 14, 17, 24, 4), PULocationID=161, DOLocationID=164),\n",
       " Row(pickup_datetime=datetime.datetime(2021, 1, 23, 1, 53, 10), dropoff_datetime=datetime.datetime(2021, 1, 23, 2, 6, 58), PULocationID=119, DOLocationID=47),\n",
       " Row(pickup_datetime=datetime.datetime(2021, 1, 14, 3, 10, 21), dropoff_datetime=datetime.datetime(2021, 1, 14, 3, 16, 53), PULocationID=36, DOLocationID=36),\n",
       " Row(pickup_datetime=datetime.datetime(2021, 1, 24, 18, 39, 50), dropoff_datetime=datetime.datetime(2021, 1, 24, 19, 0, 36), PULocationID=87, DOLocationID=188),\n",
       " Row(pickup_datetime=datetime.datetime(2021, 1, 7, 16, 33, 58), dropoff_datetime=datetime.datetime(2021, 1, 7, 16, 45, 1), PULocationID=143, DOLocationID=237),\n",
       " Row(pickup_datetime=datetime.datetime(2021, 1, 3, 2, 26, 9), dropoff_datetime=datetime.datetime(2021, 1, 3, 2, 32, 16), PULocationID=189, DOLocationID=188),\n",
       " Row(pickup_datetime=datetime.datetime(2021, 1, 21, 21, 35, 54), dropoff_datetime=datetime.datetime(2021, 1, 21, 21, 43, 43), PULocationID=254, DOLocationID=3),\n",
       " Row(pickup_datetime=datetime.datetime(2021, 1, 27, 5, 9, 55), dropoff_datetime=datetime.datetime(2021, 1, 27, 5, 21, 51), PULocationID=140, DOLocationID=41),\n",
       " Row(pickup_datetime=datetime.datetime(2021, 1, 22, 20, 33, 13), dropoff_datetime=datetime.datetime(2021, 1, 22, 20, 53, 6), PULocationID=211, DOLocationID=142),\n",
       " Row(pickup_datetime=datetime.datetime(2021, 1, 1, 7, 6, 30), dropoff_datetime=datetime.datetime(2021, 1, 1, 7, 17, 38), PULocationID=168, DOLocationID=75),\n",
       " Row(pickup_datetime=datetime.datetime(2021, 1, 6, 23, 19, 38), dropoff_datetime=datetime.datetime(2021, 1, 6, 23, 22, 59), PULocationID=180, DOLocationID=76),\n",
       " Row(pickup_datetime=datetime.datetime(2021, 1, 31, 9, 25, 6), dropoff_datetime=datetime.datetime(2021, 1, 31, 9, 28, 18), PULocationID=107, DOLocationID=79),\n",
       " Row(pickup_datetime=datetime.datetime(2021, 1, 26, 15, 55, 43), dropoff_datetime=datetime.datetime(2021, 1, 26, 16, 3, 29), PULocationID=130, DOLocationID=191),\n",
       " Row(pickup_datetime=datetime.datetime(2021, 1, 14, 14, 28, 5), dropoff_datetime=datetime.datetime(2021, 1, 14, 14, 38, 3), PULocationID=38, DOLocationID=205),\n",
       " Row(pickup_datetime=datetime.datetime(2021, 1, 10, 0, 7, 4), dropoff_datetime=datetime.datetime(2021, 1, 10, 0, 21, 38), PULocationID=240, DOLocationID=81),\n",
       " Row(pickup_datetime=datetime.datetime(2021, 1, 22, 5, 1, 52), dropoff_datetime=datetime.datetime(2021, 1, 22, 5, 12, 35), PULocationID=7, DOLocationID=129),\n",
       " Row(pickup_datetime=datetime.datetime(2021, 1, 21, 5, 35, 30), dropoff_datetime=datetime.datetime(2021, 1, 21, 5, 57, 39), PULocationID=206, DOLocationID=176),\n",
       " Row(pickup_datetime=datetime.datetime(2021, 1, 21, 14, 31, 6), dropoff_datetime=datetime.datetime(2021, 1, 21, 14, 47, 14), PULocationID=79, DOLocationID=237),\n",
       " Row(pickup_datetime=datetime.datetime(2021, 1, 25, 7, 26, 54), dropoff_datetime=datetime.datetime(2021, 1, 25, 7, 30, 18), PULocationID=141, DOLocationID=141),\n",
       " Row(pickup_datetime=datetime.datetime(2021, 1, 23, 23, 13, 20), dropoff_datetime=datetime.datetime(2021, 1, 23, 23, 31, 54), PULocationID=147, DOLocationID=18),\n",
       " Row(pickup_datetime=datetime.datetime(2021, 1, 21, 16, 39, 59), dropoff_datetime=datetime.datetime(2021, 1, 21, 17, 6, 18), PULocationID=41, DOLocationID=140),\n",
       " Row(pickup_datetime=datetime.datetime(2021, 1, 24, 20, 37, 19), dropoff_datetime=datetime.datetime(2021, 1, 24, 20, 54, 9), PULocationID=256, DOLocationID=40),\n",
       " Row(pickup_datetime=datetime.datetime(2021, 1, 12, 2, 43, 26), dropoff_datetime=datetime.datetime(2021, 1, 12, 3, 0, 24), PULocationID=18, DOLocationID=259),\n",
       " Row(pickup_datetime=datetime.datetime(2021, 1, 22, 21, 34, 16), dropoff_datetime=datetime.datetime(2021, 1, 22, 21, 41, 11), PULocationID=59, DOLocationID=147),\n",
       " Row(pickup_datetime=datetime.datetime(2021, 1, 12, 0, 39), dropoff_datetime=datetime.datetime(2021, 1, 12, 1, 16, 13), PULocationID=132, DOLocationID=265),\n",
       " Row(pickup_datetime=datetime.datetime(2021, 1, 20, 20, 35, 51), dropoff_datetime=datetime.datetime(2021, 1, 20, 20, 45, 42), PULocationID=189, DOLocationID=17),\n",
       " Row(pickup_datetime=datetime.datetime(2021, 1, 18, 7, 57, 52), dropoff_datetime=datetime.datetime(2021, 1, 18, 8, 11, 9), PULocationID=26, DOLocationID=123)]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select('pickup_datetime', 'dropoff_datetime', 'PULocationID', 'DOLocationID') \\\n",
    "  .filter(df.hvfhs_license_num == 'HV0003').take(100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0866f9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!head -n 10 head.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1b0e18",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
